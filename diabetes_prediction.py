# -*- coding: utf-8 -*-
"""Diabetes Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QvCJLAYk8WYX5fwGZ2cA3vU8uMrg3x-0

# **Diabetes Prediction using PIMA Indians Diabetes Dataset**

***Dataset Link:*** [Diabetes.csv](https://drive.google.com/file/d/1YVoanwxRKwForYGESudZW2mDz8dou9Qz/view?usp=sharing)

**Importing Required Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
from lightgbm import LGBMClassifier
import xgboost as xgb

import warnings 
warnings.filterwarnings('ignore')

"""**Getting the Information from the Dataset**"""

df_diab=pd.read_csv("/content/drive/MyDrive/Diabetes.csv")
print(df_diab.head())
print(df_diab.tail())
print(df_diab.info())

"""**Getting Visualization on Different Entries**"""

fig,axs=plt.subplots(4,2,figsize=(15,12))
axs=axs.flatten()
sns.distplot(df_diab['Pregnancies'],rug=True,color='#38b000',ax=axs[0])
sns.distplot(df_diab['Glucose'],rug=True,color='#FF9933',ax=axs[1])
sns.distplot(df_diab['BloodPressure'],rug=True,color='#522500',ax=axs[2])
sns.distplot(df_diab['SkinThickness'],rug=True,color='#66b3ff',ax=axs[3])
sns.distplot(df_diab['Insulin'],rug=True,color='#FF6699',ax=axs[4])
sns.distplot(df_diab['BMI'],color='#e76f51',rug=True,ax=axs[5])
sns.distplot(df_diab['DiabetesPedigreeFunction'],color='#03045e',rug=True,ax=axs[6])
sns.distplot(df_diab['Age'],rug=True,color='#333533',ax=axs[7])
plt.show()

"""**Pair Griding using Seaborn**"""

sns.pairplot(df_diab)

"""**Heatmap for Null Elements**"""

sns.heatmap(df_diab.isnull(),yticklabels=False)

"""**Correlation Matrix of the Dataset**"""

corr=df_diab.corr()
sns.heatmap(corr,annot=True)

"""**Creating the Models and Evaluating**

*1. Splitting the Dataset and Feature Scaling*
"""

X=df_diab.drop('Outcome',axis=1)
y=df_diab['Outcome']
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)
sc=StandardScaler()
X_train=pd.DataFrame(sc.fit_transform(X_train),columns=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age'])
X_test=pd.DataFrame(sc.fit_transform(X_test),columns=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age'])

"""*2. Model*

*2.1 Logistic Regression Model*
"""

log_params={'penalty':['l1','l2'],'C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 100],'solver':['liblinear', 'saga']} 
log_model=GridSearchCV(LogisticRegression(),log_params,cv=5)
log_model.fit(X_train,y_train)
log_predict=log_model.predict(X_test)
log_score=log_model.best_score_

"""*2.2 K-Nearest Neighbor Model*"""

knn_params={'n_neighbors':list(range(3,20,2)),'weights':['uniform','distance'],'algorithm':['auto','ball_tree','kd_tree','brute'],'metric':['euclidean','manhattan','chebyshev','minkowski']}
knn_model=GridSearchCV(KNeighborsClassifier(),knn_params,cv=5)
knn_model.fit(X_train,y_train)
knn_predict=knn_model.predict(X_test)
knn_score=knn_model.best_score_

"""*2.3 Support Vector Classifier Model*"""

svc_params={'C':[0.001, 0.01, 0.1, 1],'kernel':['linear','poly','rbf','sigmoid']}
svc_model=GridSearchCV(SVC(),svc_params,cv=5)
svc_model.fit(X_train,y_train)
svc_predict=svc_model.predict(X_test)
svc_score=svc_model.best_score_

"""*2.4 Decision Tree Classifier Model*"""

dt_params={'criterion':['gini','entropy'],'splitter':['random','best'],'max_depth': [3,5,7,9,11,13]}
dt_model=GridSearchCV(DecisionTreeClassifier(),dt_params,cv=5)
dt_model.fit(X_train,y_train)
dt_predict=dt_model.predict(X_test)
dt_score=dt_model.best_score_

"""*2.5 Random forest Classifier Model*"""

rf_params={'criterion':['gini','entropy'],'n_estimators':list(range(5,26,5)),'max_depth':list(range(3,20,2))}
rf_model=GridSearchCV(RandomForestClassifier(),rf_params,cv=5)
rf_model.fit(X_train,y_train)
rf_predict=rf_model.predict(X_test)
rf_score=rf_model.best_score_

"""*2.6 Light Gradient Boosting Classifier Model*"""

lgb_params={'n_estimators':[5,10,15,20,25,50,100],'learning_rate':[0.01,0.05,0.1],'num_leaves':[7,15,31]}
lgb_model=GridSearchCV(LGBMClassifier(),lgb_params,cv=5)
lgb_model.fit(X_train,y_train)
lgb_predict=lgb_model.predict(X_test)
lgb_score=lgb_model.best_score_

"""*2.7 eXtreme Gradient Boosting Classifier Model*"""

xgb_params={'max_depth':[3,5,7,9],'n_estimators':[5,10,15,20,25,50,100],'learning_rate': [0.01,0.05,0.1]}
xgb_model=GridSearchCV(xgb.XGBClassifier(eval_metric='logloss'), xgb_params, cv=5)
xgb_model.fit(X_train,y_train)
xgb_predict=xgb_model.predict(X_test)
xgb_score=xgb_model.best_score_

"""*3. Evaluation*"""

models=['Logistic Regression','K-Nearest Neighbor Classifier','Support Vector Classifier','Decision Tree Classifier','Random Forest Classifier','LGBM Classifier','XGB Classifier']
scores=[log_score,knn_score,svc_score,dt_score,rf_score,lgb_score,xgb_score]
score_table=pd.DataFrame({'Model':models,'Score':scores})
score_table.sort_values(by='Score',axis=0,ascending=False)
print(score_table.sort_values(by='Score',ascending=False))
sns.barplot(x=score_table['Score'],y=score_table['Model'],palette='inferno')

"""*4. Final Accuracy Percentage of all the Models*"""

print("Logistic Regression Model:",log_score*100,"%\n")
print("Support Vector Classifier Model:",svc_score*100,"%\n")
print("Random Forest Classifier Model:",rf_score*100,"%\n")
print("eXtreme Gradient Boosting Classifier Model:",xgb_score*100,"%\n")
print("Light Gradient Boosting Classifier Model:",lgb_score*100,"%\n")
print("K-Nearest Neighbor Model:",knn_score*100,"%\n")
print("Decision Tree Classifier Model:",dt_score*100,"%\n")

"""*5. Report*"""

print('Model Report','\n',metrics.classification_report(y_test,log_predict))